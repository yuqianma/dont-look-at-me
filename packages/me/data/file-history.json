{"name":"index.mjs","id":"1243000","path":"dont-look-at-me\/index.mjs","rel_path":"\/index.mjs","history":[{"created_at":"2023-06-01T04:33:32+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n"},{"created_at":"2023-06-01T16:05:32.042000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n","from_line_number":147,"to_line_number":147},{"created_at":"2023-06-01T16:05:32.070000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:32.183000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nf\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:33.513000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfu\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:33.727000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfun\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:33.974000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction name(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:34.367000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction s(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:39.105000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction sh(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:39.272000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction sho(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:39.381000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction show(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:39.534000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showD(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:40.458000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDe(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:40.742000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDet(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:41.563000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDete(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:41.702000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetec(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:42.298000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetect(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:42.556000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetecte(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:42.620000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(params) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:42.805000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(param) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:47.171000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(para) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:47.330000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(par) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:47.502000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(pa) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:47.745000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(p) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:47.789000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected() {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:48.261000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(r) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:53.526000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(re) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:53.600000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(res) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:53.794000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(resu) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:53.891000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(resul) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:53.993000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(result) {\n  \n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:05:54.109000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(result) {\n  if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:06:14.370000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(result) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":150,"to_line_number":150},{"created_at":"2023-06-01T16:06:14.412000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":148,"to_line_number":148},{"created_at":"2023-06-01T16:31:27.044000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\n\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":2,"to_line_number":2},{"created_at":"2023-06-01T16:31:27.075000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodashEs from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":2,"to_line_number":2},{"created_at":"2023-06-01T16:31:27.453000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodashE from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":2,"to_line_number":2},{"created_at":"2023-06-01T16:31:29.392000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":2,"to_line_number":2},{"created_at":"2023-06-01T16:35:29.880000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:35:29.907000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":150,"to_line_number":150},{"created_at":"2023-06-01T16:35:30.054000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nf\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:35:30.437000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfu\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:35:30.578000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfun\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:35:30.736000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfu\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:35:31.222000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nf\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:35:31.262000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:35:31.417000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":150,"to_line_number":150},{"created_at":"2023-06-01T16:35:34.429000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction showDetected(detected) {\n  if (detected) {\n    document.body.style.backgroundColor = \"#e55\";\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:35:34.643000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:52.162000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:52.596000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nf\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:52.932000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfu\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:53.071000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfun\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:53.238000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:53.386000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nf\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:53.421000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfu\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:53.458000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfun\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:53.494000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction name(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:54.343000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction d(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:54.850000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction de(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:55.022000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction (params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:55.921000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction d(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:55.961000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction de(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:55.997000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction det(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:59.452000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction dete(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:59.529000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detec(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:37:59.730000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detect(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:00.187000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detecte(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:00.318000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detected(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:01.427000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detecte(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:03.730000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detect(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:04.165000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detec(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:04.246000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction dete(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:04.348000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction det(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:04.449000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction de(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:04.548000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction d(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:04.735000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction (params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:04.921000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction d(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:54.622000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction de(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:54.828000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction det(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:55.104000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction dete(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:55.178000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detec(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:55.401000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction dete(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:56.500000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction det(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:56.645000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction de(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:56.827000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction d(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:56.998000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction (params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:57.380000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction d(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:57.595000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction de(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:57.843000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction det(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:58.344000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction dete(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:58.384000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detec(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:58.661000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detect(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:58.910000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detecte(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:58.980000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detected(params) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:38:59.211000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detected(param) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:01.538000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detected(para) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:01.671000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detected(par) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:01.814000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detected(pa) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:01.956000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detected(p) {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:02.114000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detected() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:02.339000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detecte() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:12.810000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detect() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:13.198000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction detec() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:13.360000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction dete() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:13.523000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction det() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:13.679000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction de() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:13.841000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction d() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:39:13.996000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction () {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:57.061000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction o() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:57.101000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction on() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:57.159000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onD() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:57.981000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDe() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:58.674000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDet() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:58.927000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDete() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:58.973000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetec() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:59.214000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetect() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:59.476000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetecte() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:46:59.539000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:47:05.573000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\n","from_line_number":152,"to_line_number":152},{"created_at":"2023-06-01T16:47:05.606000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\n\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:05.703000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nf\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:06.195000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfu\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:06.312000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfun\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:06.465000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction name(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:06.697000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction o(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:07.396000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction on(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:07.502000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onN(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:08.087000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNo(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:08.456000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNot(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:08.765000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotD(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:09.136000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotDe(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:09.336000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotDet(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:09.735000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotDete(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:09.775000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotDetec(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:09.979000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotDetect(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:10.187000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotDetecte(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:10.250000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotDetected(params) {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:16.936000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotDetected() {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:16.966000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction _onDetected() {\n  \n}\n\nfunction onNotDetected() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:47:20.005000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction onNotDetected() {\n  \n}\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:47:23.188000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:47:24.690000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T16:47:26.784000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\n\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:26.919000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nc\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:27.828000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nco\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:28.048000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\ncon\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:28.085000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nco\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:28.398000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nc\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:28.541000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\n\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:28.706000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nc\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:30.256000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nco\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:30.360000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\ncon\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:30.446000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\ncons\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:30.596000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:30.830000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst \n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:30.927000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst f\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:31.240000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst fu\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:31.357000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst f\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:31.747000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst \n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:31.906000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst o\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:32.342000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst on\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:32.445000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onN\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:36.655000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNo\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:37.018000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNot\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:37.408000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotD\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:38.116000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDe\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:38.637000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst _onNotDetected\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:40.578000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:43.101000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected \n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:44.295000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected =\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:44.443000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = \n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:44.564000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = l\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:45.702000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = \n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:46.115000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 1000);\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:47.298000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 100);\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:49.847000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 00);\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:49.979000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      document.body.style.backgroundColor = \"#e55\";\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T16:47:50.217000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":135,"to_line_number":135},{"created_at":"2023-06-01T16:47:56.525000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n      document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":150,"to_line_number":150},{"created_at":"2023-06-01T16:47:58.222000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n    document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":150,"to_line_number":150},{"created_at":"2023-06-01T16:47:59.173000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":150,"to_line_number":150},{"created_at":"2023-06-01T16:47:59.459000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:48:00.318000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    document.body.style.backgroundColor = \"\";\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:48:00.929000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":139,"to_line_number":139},{"created_at":"2023-06-01T16:48:03.919000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n    } else {\n      document.body.style.backgroundColor = \"\";\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:48:05.465000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n    } else {\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":136,"to_line_number":136},{"created_at":"2023-06-01T16:48:08.766000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      \n    } else {\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":135,"to_line_number":135},{"created_at":"2023-06-01T16:48:09.537000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      o\n    } else {\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":135,"to_line_number":135},{"created_at":"2023-06-01T16:48:10.868000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      on\n    } else {\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":135,"to_line_number":135},{"created_at":"2023-06-01T16:48:11.083000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected\n    } else {\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":135,"to_line_number":135},{"created_at":"2023-06-01T16:48:14.322000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":135,"to_line_number":135},{"created_at":"2023-06-01T16:48:15.092000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      \n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":137,"to_line_number":137},{"created_at":"2023-06-01T16:48:17.030000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      o\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":137,"to_line_number":137},{"created_at":"2023-06-01T16:48:17.848000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      on\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":137,"to_line_number":137},{"created_at":"2023-06-01T16:48:17.941000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    \n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":137,"to_line_number":137},{"created_at":"2023-06-01T16:48:18.501000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:49:44.699000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  \n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":150,"to_line_number":150},{"created_at":"2023-06-01T16:49:44.727000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n","from_line_number":150,"to_line_number":150},{"created_at":"2023-06-01T16:49:48.639000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T16:51:57.886000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\n\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:51:58.248000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nw\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:51:58.954000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwi\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:51:59.012000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwin\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:51:59.154000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwind\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:51:59.236000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindo\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:51:59.424000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:51:59.506000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:52:02.914000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.n\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:52:03.230000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:52:03.679000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.o\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:52:04.068000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.on\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:52:04.186000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onN\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:52:04.464000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:52:05.685000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":159,"to_line_number":159},{"created_at":"2023-06-01T16:52:28.652000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:28.683000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  c\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:29.960000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  co\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:30.071000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  con\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:30.143000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  cons\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:30.268000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  conso\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:30.401000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  consol\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:30.573000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:30.713000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:30.839000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.l\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:31.011000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.lo\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:31.206000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:31.380000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log()\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:31.642000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T16:52:32.881000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:52:42.438000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\n\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":150,"to_line_number":150},{"created_at":"2023-06-01T16:52:42.602000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nl\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:52:43.295000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nle\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:52:43.410000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:52:43.523000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet \n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:52:43.617000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":149,"to_line_number":149},{"created_at":"2023-06-01T16:52:46.659000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":152,"to_line_number":152},{"created_at":"2023-06-01T16:52:48.519000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":152,"to_line_number":152},{"created_at":"2023-06-01T16:52:50.340000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":152,"to_line_number":152},{"created_at":"2023-06-01T16:52:51.046000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":152,"to_line_number":152},{"created_at":"2023-06-01T16:52:52.504000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {}\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":152,"to_line_number":152},{"created_at":"2023-06-01T16:52:53.453000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    \n  }\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":152,"to_line_number":152},{"created_at":"2023-06-01T16:52:54.231000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T16:52:55.421000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":160,"to_line_number":160},{"created_at":"2023-06-01T16:52:57.036000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":160,"to_line_number":160},{"created_at":"2023-06-01T16:52:58.350000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {}\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":160,"to_line_number":160},{"created_at":"2023-06-01T16:52:58.824000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    \n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":160,"to_line_number":160},{"created_at":"2023-06-01T16:52:59.539000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T16:53:25.818000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  \n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:25.852000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  l\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:30.185000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  la\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:30.317000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  las\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:30.494000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:30.889000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected \n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:31.040000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected =\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:31.350000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = \n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:31.423000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = t\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:31.682000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = tr\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:31.858000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = tru\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:32.010000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:32.126000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T16:53:32.210000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":164,"to_line_number":164},{"created_at":"2023-06-01T16:53:34.090000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":164,"to_line_number":164},{"created_at":"2023-06-01T16:56:02.085000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    \n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:02.113000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    c\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:03.810000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    co\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:03.939000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    con\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:04.009000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    cons\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:04.218000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    conso\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:04.348000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    consol\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:04.723000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:04.891000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:05.038000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.l\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:05.229000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.lo\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:05.740000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:06.060000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log()\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:06.357000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\")\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:07.438000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\"  )\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:08.243000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\" )\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:08.933000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\")\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:09.733000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":140,"to_line_number":140},{"created_at":"2023-06-01T16:56:52.438000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":162,"to_line_number":162},{"created_at":"2023-06-01T17:03:22.015000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:22.050000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:22.714000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:23.305000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.c\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:24.095000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.ca\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:24.216000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.can\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:25.041000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.canc\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:25.084000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cance\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:25.295000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:25.604000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel()\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:03:26.044000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:05:39.805000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:06:18.695000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500,);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:18.723000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, );\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:19.423000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, {});\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:20.486000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:21.242000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, {  });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:21.601000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { m });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:21.895000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { ma });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:21.953000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { max });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:22.116000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxW });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:25.164000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWa });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:25.426000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 1000 } });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:26.205000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 1000  });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:27.451000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 1000 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:27.934000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 100 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:30.024000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 00 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:06:30.186000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":171,"to_line_number":171},{"created_at":"2023-06-01T17:07:37.362000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \/\/ onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":163,"to_line_number":163},{"created_at":"2023-06-01T17:07:45.102000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:10:10.119000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:10:10.156000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\n\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:10:10.265000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nc\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:11.047000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nco\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:11.147000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\ncon\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:11.246000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\ncons\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:11.322000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:11.475000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst \n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:11.575000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst E\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:13.436000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EY\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:13.662000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:13.854000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:15.875000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_D\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:16.779000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DE\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:17.337000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DET\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:23.176000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETE\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:23.667000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETEC\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:24.008000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECT\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:24.947000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTI\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:25.199000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTIO\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:25.340000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:25.595000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:25.916000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_T\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:26.287000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TI\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:26.786000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIM\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:27.022000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIME\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:27.631000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, 500, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:10:28.282000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, , { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:10:39.377000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: 500 });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:10:40.231000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.5);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:15:34.017000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":135,"to_line_number":135},{"created_at":"2023-06-01T17:15:34.052000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":135,"to_line_number":135},{"created_at":"2023-06-01T17:15:58.023000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT,);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:15:58.058000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:19:57.385000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":155,"to_line_number":155},{"created_at":"2023-06-01T17:19:57.416000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":165,"to_line_number":165},{"created_at":"2023-06-01T17:22:09.578000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT,);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:22:09.612000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, );\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:22:09.725000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, {);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:22:11.494000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { );\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:22:12.994000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { m);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:22:15.891000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { ma);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:22:15.930000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { max);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:22:16.105000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxW);\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:22:16.988000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:37:09.687000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\n\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":154,"to_line_number":154},{"created_at":"2023-06-01T17:37:09.716000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nl\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:10.578000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nle\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:10.737000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:11.175000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet \n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:11.293000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet e\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:11.872000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet ey\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:12.052000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eye\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:12.187000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeout = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:15.297000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutI = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:30.541000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":153,"to_line_number":153},{"created_at":"2023-06-01T17:37:31.038000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NDETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:47.361000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NoDETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:48.533000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NDETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:49.187000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NODETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:49.820000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NONDETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:50.017000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_TIMEOUT = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:50.840000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_TIMEOU = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:54.152000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_TIMEO = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:54.286000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_TIME = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:54.416000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_TIM = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:54.574000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_TI = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:54.720000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_T = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:54.856000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_ = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:55.011000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_D = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:55.483000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DE = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:55.741000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DEL = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:56.061000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELA = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:56.923000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nconst onNotDetected = lodash.debounce(_onNotDetected, EYE_DETECTION_TIMEOUT, { maxWait: EYE_DETECTION_TIMEOUT });\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":5,"to_line_number":5},{"created_at":"2023-06-01T17:37:57.201000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:38:03.266000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nwindow.onNotDetected = onNotDetected;\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:38:04.538000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:38:05.039000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  onNotDetected.cancel();\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n","from_line_number":165,"to_line_number":165},{"created_at":"2023-06-01T17:38:07.180000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction onNotDetected() {\n  \/\/ if (lastDetected === false) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T17:38:13.169000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n","from_line_number":165,"to_line_number":165},{"created_at":"2023-06-01T17:38:48.866000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n","from_line_number":164,"to_line_number":164},{"created_at":"2023-06-01T17:38:52.607000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:38:54.061000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:38:54.181000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nf\n\n","from_line_number":173,"to_line_number":174},{"created_at":"2023-06-01T17:38:54.732000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:38:56.038000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {}\n\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:38:56.468000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  \n}\n\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:38:57.074000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:39:10.276000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:39:21.057000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:39:21.582000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    r\n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:39:22.023000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    re\n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:39:22.423000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    ret\n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:39:22.661000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    retu\n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:39:22.705000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    retur\n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:39:22.786000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return\n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:39:22.883000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(_onNotDetected, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:39:23.034000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":177,"to_line_number":177},{"created_at":"2023-06-01T17:39:46.563000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout((), EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":177,"to_line_number":177},{"created_at":"2023-06-01T17:39:49.075000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() , EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":177,"to_line_number":177},{"created_at":"2023-06-01T17:39:49.425000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() =, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":177,"to_line_number":177},{"created_at":"2023-06-01T17:39:49.694000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() =>, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":177,"to_line_number":177},{"created_at":"2023-06-01T17:39:50.032000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => , EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":177,"to_line_number":177},{"created_at":"2023-06-01T17:39:50.217000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {}, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":177,"to_line_number":177},{"created_at":"2023-06-01T17:39:50.478000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    \n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":177,"to_line_number":177},{"created_at":"2023-06-01T17:39:50.991000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:39:52.993000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:40:07.131000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n\/\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:40:07.414000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\n\/\/\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:40:07.595000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":173,"to_line_number":173},{"created_at":"2023-06-01T17:40:10.678000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    \n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:40:10.941000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:40:11.902000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  \n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:40:14.023000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  \/\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:40:14.300000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  \/\/\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:40:14.490000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  \/\/ \n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:40:20.564000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  \/\/\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:40:25.333000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  \/\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:40:25.527000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  \n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:40:25.682000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:40:25.850000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":174,"to_line_number":174},{"created_at":"2023-06-01T17:40:26.210000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:27.475000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:27.781000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:27.915000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:30.226000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:42.046000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:42.356000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:42.509000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ \n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:42.711000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ c\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:47.880000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ co\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:48.010000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ cou\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:48.098000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \/\/ if (lastDetected === true) {\n  \/\/   return;\n  \/\/ }\n  \/\/ lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":175,"to_line_number":175},{"created_at":"2023-06-01T17:40:50.048000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T17:40:59.997000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  \n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T17:41:03.634000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  c\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T17:41:11.225000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  ca\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T17:41:11.355000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  c\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T17:41:12.635000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  cl\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T17:41:12.858000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  cle\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T17:41:13.144000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T17:41:14.890000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":161,"to_line_number":161},{"created_at":"2023-06-01T17:41:15.641000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  \n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":162,"to_line_number":162},{"created_at":"2023-06-01T17:41:20.925000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":162,"to_line_number":162},{"created_at":"2023-06-01T17:46:26.503000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    \n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:26.532000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    c\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:27.121000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    co\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:27.338000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    con\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:27.405000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    cons\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:27.458000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    conso\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:27.637000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    consol\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:28.149000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:28.312000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:28.482000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.l\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:28.682000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.lo\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:28.926000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:29.223000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log()\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:29.612000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\")\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:46:30.784000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":178,"to_line_number":178},{"created_at":"2023-06-01T17:50:17.224000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  \n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T17:50:17.261000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  i\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T17:50:18.013000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T17:50:18.140000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if \n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T17:50:18.227000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if ()\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T17:50:18.450000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T17:50:19.633000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {}\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T17:50:20.094000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    \n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":156,"to_line_number":156},{"created_at":"2023-06-01T17:50:20.662000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    c\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T17:50:24.478000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    cl\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T17:50:24.760000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    cle\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T17:50:25.135000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T17:50:26.321000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":157,"to_line_number":157},{"created_at":"2023-06-01T17:50:26.875000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    \n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:50:27.624000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  clearTimeout(eyeDetectionTimeoutId);\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":158,"to_line_number":158},{"created_at":"2023-06-01T17:50:28.696000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  eyeDetectionTimeoutId = undefined;\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":165,"to_line_number":165},{"created_at":"2023-06-01T17:50:30.772000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":165,"to_line_number":165},{"created_at":"2023-06-01T17:53:01.238000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":169,"to_line_number":169},{"created_at":"2023-06-01T17:53:01.267000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":169,"to_line_number":169},{"created_at":"2023-06-01T17:53:05.382000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() { \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":168,"to_line_number":168},{"created_at":"2023-06-01T17:53:05.777000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() { \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  \n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":179,"to_line_number":179},{"created_at":"2023-06-01T17:53:23.611000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() { \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":179,"to_line_number":179},{"created_at":"2023-06-01T17:53:23.942000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() { \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  \n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":179,"to_line_number":179},{"created_at":"2023-06-01T17:53:52.158000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() { \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":179,"to_line_number":179},{"created_at":"2023-06-01T17:53:52.528000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":168,"to_line_number":168},{"created_at":"2023-06-01T17:53:52.957000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  \n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":169,"to_line_number":169},{"created_at":"2023-06-01T17:53:53.856000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":169,"to_line_number":169},{"created_at":"2023-06-01T17:53:54.335000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    \/\/ console.log(\"countdown is running\");\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":180,"to_line_number":180},{"created_at":"2023-06-01T17:54:02.889000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nimport lodash from 'https:\/\/cdn.jsdelivr.net\/npm\/lodash-es@4.17.21\/+esm'\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":180,"to_line_number":180},{"created_at":"2023-06-01T17:54:23.151000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NON_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NON_DETECTION_DELAY);\n}\n\n","from_line_number":2,"to_line_number":2},{"created_at":"2023-06-01T17:56:03.986000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n","from_line_number":4,"to_line_number":184},{"created_at":"2023-06-01T17:56:04.021000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NO_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:56:07.730000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 1000;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:56:56.669000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 100;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:56:56.699000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 00;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:56:56.809000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:57:28.327000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 5g00;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:57:28.365000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 5gi00;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:57:28.503000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n","from_line_number":4,"to_line_number":4},{"created_at":"2023-06-01T17:57:30.115000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\n","from_line_number":187,"to_line_number":187},{"created_at":"2023-06-02T05:26:38.849000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\n\n","from_line_number":188,"to_line_number":188},{"created_at":"2023-06-02T05:26:38.985000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: '..\/node_modules\/monaco-editor\/min\/vs' } });\n\n\t\t\trequire(['vs\/editor\/editor.main'], function () {\n\t\t\t\tvar editor = monaco.editor.create(document.getElementById('container'), {\n\t\t\t\t\tvalue: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n\t\t\t\t\tlanguage: 'javascript'\n\t\t\t\t});\n\t\t\t});\n","from_line_number":188,"to_line_number":189},{"created_at":"2023-06-02T05:26:40.005000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: '..\/node_modules\/monaco-editor\/min\/vs' } });\n\n    require(['vs\/editor\/editor.main'], function () {\n      var editor = monaco.editor.create(document.getElementById('container'), {\n        value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n        language: 'javascript'\n      });\n    });\n","from_line_number":190,"to_line_number":190},{"created_at":"2023-06-02T05:26:44.036000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: '..\/node_modules\/monaco-editor\/min\/vs' } });\n\n  require(['vs\/editor\/editor.main'], function () {\n    var editor = monaco.editor.create(document.getElementById('container'), {\n      value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n      language: 'javascript'\n    });\n  });\n","from_line_number":190,"to_line_number":196},{"created_at":"2023-06-02T05:26:44.270000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: '..\/node_modules\/monaco-editor\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('container'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript'\n  });\n});\n","from_line_number":190,"to_line_number":196},{"created_at":"2023-06-02T05:27:24.626000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'x' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('container'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript'\n  });\n});\n","from_line_number":188,"to_line_number":188},{"created_at":"2023-06-02T05:27:24.653000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: '' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('container'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript'\n  });\n});\n","from_line_number":188,"to_line_number":188},{"created_at":"2023-06-02T05:27:25.551000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs\/' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('container'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript'\n  });\n});\n","from_line_number":188,"to_line_number":188},{"created_at":"2023-06-02T05:28:18.697000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('container'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript'\n  });\n});\n","from_line_number":188,"to_line_number":188},{"created_at":"2023-06-02T05:29:53.396000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('container'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n  });\n});\n","from_line_number":193,"to_line_number":193},{"created_at":"2023-06-02T05:29:53.432000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('container'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n    \n  });\n});\n","from_line_number":194,"to_line_number":194},{"created_at":"2023-06-02T05:29:53.983000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('container'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n    theme: \"vs-dark\",\n  });\n});\n","from_line_number":194,"to_line_number":194},{"created_at":"2023-06-02T05:30:32.507000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('econtainer'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n    theme: \"vs-dark\",\n  });\n});\n","from_line_number":191,"to_line_number":191},{"created_at":"2023-06-02T05:30:32.534000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('edcontainer'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n    theme: \"vs-dark\",\n  });\n});\n","from_line_number":191,"to_line_number":191},{"created_at":"2023-06-02T05:30:32.766000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('edicontainer'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n    theme: \"vs-dark\",\n  });\n});\n","from_line_number":191,"to_line_number":191},{"created_at":"2023-06-02T05:30:32.943000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('editcontainer'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n    theme: \"vs-dark\",\n  });\n});\n","from_line_number":191,"to_line_number":191},{"created_at":"2023-06-02T05:30:33.090000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('editocontainer'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n    theme: \"vs-dark\",\n  });\n});\n","from_line_number":191,"to_line_number":191},{"created_at":"2023-06-02T05:30:33.160000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('editorcontainer'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n    theme: \"vs-dark\",\n  });\n});\n","from_line_number":191,"to_line_number":191},{"created_at":"2023-06-02T05:30:33.267000+00:00","content":"import vision from \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\";\nconst { FaceLandmarker, FilesetResolver, DrawingUtils } = vision;\n\nconst EYE_NOT_DETECTION_DELAY = 500;\n\nlet faceLandmarker;\n let webcamRunning = false;\nconst videoWidth = 480;\n\n\/\/ Before we can use HandLandmarker class we must wait for it to finish\n\/\/ loading. Machine Learning models can be large and take a moment to\n\/\/ get everything needed to run.\nasync function runDemo() {\n  \/\/ Read more `CopyWebpackPlugin`, copy wasm set from \"https:\/\/cdn.skypack.dev\/node_modules\" to `\/wasm`\n  const filesetResolver = await FilesetResolver.forVisionTasks(\n    \"https:\/\/cdn.jsdelivr.net\/npm\/@mediapipe\/tasks-vision@0.10.0\/wasm\"\n  );\n  faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {\n    baseOptions: {\n      modelAssetPath: `https:\/\/storage.googleapis.com\/mediapipe-models\/face_landmarker\/face_landmarker\/float16\/1\/face_landmarker.task`,\n      delegate: \"GPU\"\n    },\n    outputFaceBlendshapes: true,\n    runningMode: \"VIDEO\",\n    numFaces: 1\n  });\n\n\t\/\/ enableCam();\n}\nrunDemo();\n\nconst video = document.getElementById(\"webcam\");\nconst canvasElement = document.getElementById(\n  \"output_canvas\"\n);\n\nconst canvasCtx = canvasElement.getContext(\"2d\");\n\ndocument.addEventListener(\"keydown\", (event) => {\n\tif (event.code === \"Space\") {\n\t\tenableCam();\n\t}\n});\n\n\/\/ Enable the live webcam view and start detection.\n function enableCam(event) {\n  if (!faceLandmarker) {\n    console.log(\"Wait! faceLandmarker not loaded yet.\");\n    return;\n  }\n\n  if (webcamRunning === true) {\n    webcamRunning = false;\n  } else {\n    webcamRunning = true;\n  }\n\n  \/\/ getUsermedia parameters.\n  const constraints = {\n    video: true\n  };\n\n  \/\/ Activate the webcam stream.\n  navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {\n    video.srcObject = stream;\n    video.addEventListener(\"loadeddata\", predictWebcam);\n  });\n}\n\nlet lastVideoTime = -1;\nlet results = undefined;\nconst drawingUtils = new DrawingUtils(canvasCtx);\nasync function predictWebcam() {\n  const radio = video.videoHeight \/ video.videoWidth;\n  video.style.width = videoWidth + \"px\";\n  video.style.height = videoWidth * radio + \"px\";\n  canvasElement.style.width = videoWidth + \"px\";\n  canvasElement.style.height = videoWidth * radio + \"px\";\n  canvasElement.width = video.videoWidth;\n  canvasElement.height = video.videoHeight;\n  \n  let nowInMs = Date.now();\n  if (lastVideoTime !== video.currentTime) {\n    lastVideoTime = video.currentTime;\n    results = faceLandmarker.detectForVideo(video, nowInMs);\n\t\twindow._results = results;\n  }\n  if (results.faceLandmarks) {\n    for (const landmarks of results.faceLandmarks) {\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_EYE,\n        { color: \"#30FF30\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_RIGHT_IRIS,\n        { color: \"#FF3030\" }\n      );\n      drawingUtils.drawConnectors(\n        landmarks,\n        FaceLandmarker.FACE_LANDMARKS_LEFT_IRIS,\n        { color: \"#30FF30\" }\n      );\n    }\n  }\n\n  \/\/ display faceBlendshapes in document\n  if (results.faceBlendshapes[0]) {\n    const faceBlendshapesElement = document.getElementById(\n      \"faceBlendshapes\"\n    );\n    const faceBlendshapes = results.faceBlendshapes[0].categories;\n    let htmlMaker = \"\";\n    faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eye\")).forEach((shape) => {\n      htmlMaker += `\n      <li class=\"blend-shapes-item\">\n        <span class=\"blend-shapes-label\">${\n          shape.displayName || shape.categoryName\n        }<\/span>\n        <span class=\"blend-shapes-value\" style=\"width: calc(${\n          +shape.score * 100\n        }% - 120px)\">${(+shape.score).toFixed(4)}<\/span>\n      <\/li>\n    `;\n    });\n    faceBlendshapesElement.innerHTML = htmlMaker;\n\n    const detected = faceBlendshapes.filter((s) => s.categoryName.startsWith(\"eyeLook\")).every((shape) => shape.score < 0.6);\n    if (detected) {\n      onDetected();\n    } else {\n      onNotDetected();\n    }\n  } else {\n    console.log(\"no faceBlendshapes\");\n    onNotDetected();\n  }\n\n  \/\/ Call this function again to keep predicting when the browser is ready.\n  if (webcamRunning === true) {\n    window.requestAnimationFrame(predictWebcam);\n  }\n}\n\nlet lastDetected = false;\nlet eyeDetectionTimeoutId = undefined;\n\nfunction onDetected() {\n  if (eyeDetectionTimeoutId) {\n    clearTimeout(eyeDetectionTimeoutId);\n    eyeDetectionTimeoutId = undefined;\n  }\n  if (lastDetected === true) {\n    return;\n  }\n  lastDetected = true;\n  console.log(\"detected\");\n  document.body.style.backgroundColor = \"#e55\";\n}\n\nfunction _onNotDetected() {\n  if (lastDetected === false) {\n    return;\n  }\n  lastDetected = false;\n  console.log(\"not detected\");\n  document.body.style.backgroundColor = \"\";\n}\n\nfunction onNotDetected() {\n  if (eyeDetectionTimeoutId) {\n    \/\/ countdown is running\n    return;\n  }\n  eyeDetectionTimeoutId = setTimeout(() => {\n    eyeDetectionTimeoutId = undefined;\n    _onNotDetected();\n  }, EYE_NOT_DETECTION_DELAY);\n}\n\n\nrequire.config({ paths: { vs: 'https:\/\/cdn.jsdelivr.net\/npm\/monaco-editor@0.38.0\/min\/vs' } });\n\nrequire(['vs\/editor\/editor.main'], function () {\n  var editor = monaco.editor.create(document.getElementById('editor-container'), {\n    value: ['function x() {', '\\tconsole.log(\"Hello world!\");', '}'].join('\\n'),\n    language: 'javascript',\n    theme: \"vs-dark\",\n  });\n});\n","from_line_number":191,"to_line_number":191}]}